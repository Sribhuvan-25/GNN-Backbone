#!/usr/bin/env python3

"""
Case 1 Pipeline with PROPER Graph Visualizations
- Initial k-NN graph visualization
- Edge-based sparsified graph visualization  
- Node-based sparsified graph visualization
- All new features properly integrated
"""

import os
import sys
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from torch import nn
from torch.optim import Adam, lr_scheduler
from torch_geometric.loader import DataLoader
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.svm import LinearSVR
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import torch.nn.functional as F
from torch_geometric.data import Data
import pickle
import warnings
warnings.filterwarnings('ignore')

# Add src to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Import our modules - using working versions with all fixes
from datasets.dataset_regression_working import MicrobialGNNDataset
from explainers.explainer_regression_working import GNNExplainerRegression  
from explainers.pipeline_explainer_working import create_explainer_sparsified_graph
from models.GNNmodelsRegression import (
    simple_GCN_res_plus_regression,
    simple_RGGC_plus_regression,
    simple_GAT_regression
)

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Reproducibility
torch.manual_seed(42)
np.random.seed(42)

class Case1PipelineWithGraphs:
    """Case 1: H2-km prediction with COMPLETE graph visualizations"""
    
    def __init__(self, data_path, save_dir="./case1_with_graphs_results"):
        self.data_path = data_path
        self.save_dir = save_dir
        
        # Create directories
        os.makedirs(f"{save_dir}/plots", exist_ok=True)
        os.makedirs(f"{save_dir}/models", exist_ok=True)
        os.makedirs(f"{save_dir}/embeddings", exist_ok=True)
        os.makedirs(f"{save_dir}/graphs", exist_ok=True)
        
        # Initialize pipeline attributes needed for explainer
        self.graph_mode = 'family'  # Pipeline graph mode
        self.save_dir = save_dir    # Pipeline save directory
        
        # Fast parameters for testing
        self.k_neighbors = 10  # Use same as working version
        self.hidden_dim = 256
        self.num_epochs = 50  # Very fast for testing
        self.num_folds = 5    # Fast
        self.batch_size = 4
        self.learning_rate = 0.01
        
        print(f"Case 1 Pipeline initialized - Results will be saved to: {save_dir}")
    
    def create_anchored_dataset(self):
        """Create dataset with Case 1 anchored features"""
        print("\n" + "="*60)
        print("STEP 1: Creating Case 1 Dataset with Anchored Features")
        print("="*60)
        
        # Case 1: Hydrogenotrophic families
        anchored_families = [
            'Methanoregulaceae',      # Hydrogenotrophic
            'Methanobacteriaceae',    # Hydrogenotrophic  
            'Methanospirillaceae'     # Hydrogenotrophic
        ]
        
        print(f"Case 1 anchored families: {anchored_families}")
        
        # Create base dataset
        self.dataset = MicrobialGNNDataset(
            data_path=self.data_path,
            k_neighbors=self.k_neighbors,
            mantel_threshold=0.05,
            use_fast_correlation=False,  # Use Mantel test for proper filtering
            graph_mode='family',
            family_filter_mode='strict'  # Use strict filtering to get ~20-30 families like working version
        )
        
        print(f"Dataset created with {len(self.dataset.data_list)} samples")
        print(f"Target variables: {self.dataset.target_cols}")
        print(f"Number of features: {len(self.dataset.node_feature_names)}")
        
        # IMPORTANT: Store original graph data for visualization
        self.dataset.original_graph_data = {
            'edge_index': self.dataset.data_list[0].edge_index.clone(),
            'edge_weight': torch.ones(self.dataset.data_list[0].edge_index.shape[1]),
            'edge_type': torch.ones(self.dataset.data_list[0].edge_index.shape[1], dtype=torch.long)
        }
        
        print(f"Original k-NN graph stored: {self.dataset.original_graph_data['edge_index'].shape[1]} edges")
        
        return self.dataset
    
    def train_gnn_models(self, data_list, target_idx=1):  # H2-km is index 1
        """Train GNN models on the data"""
        print(f"\n" + "="*60)
        print(f"STEP 2: Training GNN Models for {self.dataset.target_cols[target_idx]}")
        print("="*60)
        
        models_to_train = ['gcn']  # Just GCN for speed
        results = {}
        
        for model_type in models_to_train:
            print(f"\nTraining {model_type.upper()}...")
            
            # Create model
            model = simple_GCN_res_plus_regression(
                hidden_channels=self.hidden_dim,
                dropout_prob=0.3,
                input_channel=1,
                output_dim=1
            ).to(device)
            
            # Train with cross-validation
            kf = KFold(n_splits=self.num_folds, shuffle=True, random_state=42)
            fold_results = []
            best_model_state = None
            best_r2 = -float('inf')
            
            for fold, (train_idx, test_idx) in enumerate(kf.split(data_list), 1):
                print(f"  Fold {fold}/{self.num_folds}")
                
                train_data = [data_list[i] for i in train_idx]
                test_data = [data_list[i] for i in test_idx]
                
                train_loader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)
                test_loader = DataLoader(test_data, batch_size=self.batch_size, shuffle=False)
                
                optimizer = Adam(model.parameters(), lr=self.learning_rate)
                criterion = nn.MSELoss()
                
                # Training loop
                for epoch in range(self.num_epochs):
                    model.train()
                    for batch in train_loader:
                        batch = batch.to(device)
                        optimizer.zero_grad()
                        out, _ = model(batch.x, batch.edge_index, batch.batch)
                        target = batch.y[:, target_idx].view(-1, 1)
                        loss = criterion(out, target)
                        loss.backward()
                        optimizer.step()
                
                # Validation
                model.eval()
                preds, trues = [], []
                with torch.no_grad():
                    for batch in test_loader:
                        batch = batch.to(device)
                        out, _ = model(batch.x, batch.edge_index, batch.batch)
                        preds.append(out.cpu().numpy())
                        trues.append(batch.y[:, target_idx].cpu().numpy())
                
                preds = np.concatenate(preds).flatten()
                trues = np.concatenate(trues).flatten()
                
                r2 = r2_score(trues, preds)
                rmse = np.sqrt(mean_squared_error(trues, preds))
                
                fold_results.append({
                    'fold': fold,
                    'r2': r2,
                    'rmse': rmse,
                    'predictions': preds,
                    'targets': trues
                })
                
                if r2 > best_r2:
                    best_r2 = r2
                    best_model_state = model.state_dict().copy()
                
                print(f"    Fold {fold}: RÂ² = {r2:.3f}, RMSE = {rmse:.3f}")
            
            # Load best model
            model.load_state_dict(best_model_state)
            
            overall_r2 = np.mean([f['r2'] for f in fold_results])
            overall_rmse = np.mean([f['rmse'] for f in fold_results])
            
            print(f"  {model_type.upper()} Overall: RÂ² = {overall_r2:.3f}, RMSE = {overall_rmse:.3f}")
            
            results[model_type] = {
                'model': model,
                'fold_results': fold_results,
                'r2': overall_r2,
                'rmse': overall_rmse
            }
        
        # Get best model
        best_model = results['gcn']['model']
        
        return results, best_model
    
    def create_both_sparsified_graphs(self, model, target_idx=1):
        """Create BOTH edge-based and node-based sparsified graphs"""
        print(f"\n" + "="*60)
        print("STEP 3: Creating BOTH Sparsification Types")
        print("="*60)
        
        # Create explainer
        explainer = GNNExplainerRegression(model, device)
        
        # Generate explanations from multiple samples
        print("Generating explanations...")
        num_samples = min(5, len(self.dataset.data_list))
        combined_importance = torch.zeros(
            (len(self.dataset.node_feature_names), len(self.dataset.node_feature_names)), 
            device=device
        )
        
        for i in range(num_samples):
            data = self.dataset.data_list[i]
            edge_importance_matrix, _ = explainer.explain_graph(
                data, 
                node_names=self.dataset.node_feature_names,
                target_idx=target_idx
            )
            combined_importance += edge_importance_matrix
        
        combined_importance /= num_samples
        
        print(f"\nğŸ†• NEW FEATURE 1: Node Importance Reporting")
        print("-" * 50)
        
        # NEW FEATURE 1: Node importance reporting
        node_importance, sorted_indices = explainer.get_node_importance(
            combined_importance, 
            self.dataset.node_feature_names
        )
        
        print(f"\nğŸ†• STEP 3.1: Creating NODE-BASED Pruned Graph (Remove Nodes)")
        print("-" * 50)
        
        # STEP 1: Create NODE-BASED pruning (remove nodes from original graph)
        template_data = self.dataset.data_list[0]
        pruned_data, kept_nodes, pruned_node_names = explainer.create_node_pruned_graph(
            template_data,
            combined_importance,
            self.dataset.node_feature_names,
            importance_threshold=0.2,
            min_nodes=10  # Keep reasonable number of nodes
        )
        
        # Create node-based dataset
        node_based_data = []
        feature_matrix_samples = self.dataset.feature_matrix.T
        
        for s in range(feature_matrix_samples.shape[0]):
            x = torch.tensor(feature_matrix_samples[s][kept_nodes], dtype=torch.float32).view(-1, 1)
            targets = torch.tensor(self.dataset.target_df.iloc[s].values, dtype=torch.float32).view(1, -1)
            
            data = Data(
                x=x,
                edge_index=pruned_data.edge_index.clone(),
                y=targets
            )
            node_based_data.append(data)
        
        # Extract actual edge weights for the pruned graph edges
        node_based_edge_weights = []
        for i in range(pruned_data.edge_index.shape[1]):
            # Map back to original node indices to get importance from combined_importance matrix
            u_new, v_new = pruned_data.edge_index[0, i].item(), pruned_data.edge_index[1, i].item()
            u_orig, v_orig = kept_nodes[u_new], kept_nodes[v_new]
            
            # Get the edge importance value from the combined matrix (absolute value)
            edge_weight = abs(combined_importance[u_orig, v_orig].item())
            node_based_edge_weights.append(edge_weight)
        
        # Store node-based graph data for visualization with actual edge weights
        node_based_graph_data = {
            'edge_index': pruned_data.edge_index.clone(),
            'edge_weight': torch.tensor(node_based_edge_weights, dtype=torch.float32),
            'edge_type': torch.ones(pruned_data.edge_index.shape[1], dtype=torch.long),
            'pruning_type': 'node_based',
            'kept_nodes': kept_nodes,
            'pruned_node_names': pruned_node_names
        }
        
        print(f"\nğŸ“Š STEP 3.2: Creating EDGE-BASED Sparsification (Remove Edges from Node-Pruned Graph)")
        print("-" * 50)
        
        # STEP 2: Apply edge sparsification to the NODE-PRUNED graph
        edge_sparsified_data, edge_weights, edge_types = explainer.create_edge_sparsified_graph(
            pruned_data,  # Use the node-pruned graph as input
            combined_importance,
            pruned_node_names,  # Use pruned node names
            edge_threshold=0.15,
            kept_nodes=kept_nodes
        )
        
        # Create final edge-sparsified dataset
        edge_based_data = []
        for s in range(feature_matrix_samples.shape[0]):
            x = torch.tensor(feature_matrix_samples[s][kept_nodes], dtype=torch.float32).view(-1, 1)
            targets = torch.tensor(self.dataset.target_df.iloc[s].values, dtype=torch.float32).view(1, -1)
            
            data = Data(
                x=x,
                edge_index=edge_sparsified_data.edge_index.clone(),
                edge_weight=edge_weights,
                y=targets
            )
            edge_based_data.append(data)
        
        # Store edge-based graph data for visualization
        edge_based_graph_data = {
            'edge_index': edge_sparsified_data.edge_index.clone(),
            'edge_weight': edge_weights,
            'edge_type': edge_types,
            'pruning_type': 'node_then_edge',
            'kept_nodes': kept_nodes,
            'pruned_node_names': pruned_node_names
        }
        
        return edge_based_data, edge_based_graph_data, node_based_data, node_based_graph_data, kept_nodes, pruned_node_names
    
    def create_comprehensive_graph_visualizations(self, edge_based_graph_data, node_based_graph_data):
        """Create comprehensive graph visualizations"""
        print(f"\n" + "="*60)
        print("ğŸ¨ STEP 4: Creating Comprehensive Graph Visualizations")
        print("="*60)
        
        # Create a figure with three subplots
        fig, axes = plt.subplots(1, 3, figsize=(30, 10))
        
        num_original_nodes = len(self.dataset.node_feature_names)
        
        # 1. Original k-NN graph
        original_edges = self.dataset.original_graph_data['edge_index'].shape[1] // 2
        self._visualize_single_graph(
            self.dataset.original_graph_data['edge_index'],
            self.dataset.original_graph_data['edge_weight'],
            self.dataset.original_graph_data['edge_type'],
            axes[0],
            title=f"Original k-NN Graph\n({num_original_nodes} nodes, {original_edges} edges)",
            node_names=self.dataset.node_feature_names
        )
        
        # 2. Node-based pruned graph (STEP 1: Node pruning)
        pruned_node_names = node_based_graph_data.get('pruned_node_names', self.dataset.node_feature_names)
        node_based_nodes = len(pruned_node_names)
        node_based_edges = node_based_graph_data['edge_index'].shape[1] // 2
        node_retention = (node_based_nodes / num_original_nodes * 100)
        edge_retention_node = (node_based_edges / original_edges * 100) if original_edges > 0 else 0
        self._visualize_single_graph(
            node_based_graph_data['edge_index'],
            node_based_graph_data['edge_weight'],
            node_based_graph_data['edge_type'],
            axes[1],
            title=f"Node-Based Pruned (Step 1)\n({node_based_nodes} nodes, {node_based_edges} edges)\n({node_retention:.1f}% nodes retained)",
            node_names=pruned_node_names,
            show_edge_weights=True
        )
        
        # 3. Edge-based sparsified graph (STEP 2: Edge sparsification on node-pruned graph)
        edge_based_edges = edge_based_graph_data['edge_index'].shape[1] // 2
        edge_retention = (edge_based_edges / node_based_edges * 100) if node_based_edges > 0 else 0
        self._visualize_single_graph(
            edge_based_graph_data['edge_index'],
            edge_based_graph_data['edge_weight'],
            edge_based_graph_data['edge_type'],
            axes[2],
            title=f"Edge-Based Sparsified (Step 2)\n({node_based_nodes} nodes, {edge_based_edges} edges)\n({edge_retention:.1f}% edges retained from Step 1)",
            node_names=pruned_node_names
        )
        
        plt.tight_layout()
        graph_comparison_path = f"{self.save_dir}/graphs/comprehensive_graph_comparison.png"
        plt.savefig(graph_comparison_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… Comprehensive graph comparison saved: {graph_comparison_path}")
        
        # Create individual high-res graphs
        self._save_individual_graph_plots()
        
        return graph_comparison_path
    
    def _visualize_single_graph(self, edge_index, edge_weight, edge_type, ax, title, node_names=None, show_edge_weights=False):
        """Visualize a single graph with proper node names and consistent styling"""
        import networkx as nx
        
        # Convert to NetworkX
        G = nx.Graph()
        
        # Add nodes with proper names
        num_nodes = edge_index.max().item() + 1 if edge_index.shape[1] > 0 else 0
        
        # Use actual node names if available
        if node_names is not None and len(node_names) >= num_nodes:
            for i in range(num_nodes):
                G.add_node(i, name=node_names[i])
        else:
            # Fallback to generic node names
            for i in range(num_nodes):
                G.add_node(i, name=f"Node_{i}")
        
        # Add edges with weights
        edge_list = []
        weights = []
        for i in range(edge_index.shape[1]):
            u, v = edge_index[0, i].item(), edge_index[1, i].item()
            if u != v:  # Avoid self-loops
                weight = edge_weight[i].item() if len(edge_weight) > i else 1.0
                edge_list.append((u, v))
                weights.append(weight)
                G.add_edge(u, v, weight=weight)
        
        if len(edge_list) == 0:
            ax.text(0.5, 0.5, "No edges to display", ha='center', va='center', fontsize=14)
            ax.set_title(title)
            ax.axis('off')
            return
        
        # Create layout
        try:
            pos = nx.spring_layout(G, k=1, iterations=50, seed=42)
        except:
            pos = {i: (np.random.random(), np.random.random()) for i in G.nodes()}
        
        # Normalize edge weights for consistent visualization
        if weights:
            min_weight = min(weights)
            max_weight = max(weights)
            if max_weight > min_weight:
                normalized_weights = [(w - min_weight) / (max_weight - min_weight) * 3 + 0.5 for w in weights]
            else:
                normalized_weights = [1.0] * len(weights)
        else:
            normalized_weights = [1.0]
        
        # Draw nodes with consistent sizing
        node_size = max(10, 300 - len(G.nodes()) // 10)  # Scale node size based on graph size
        nx.draw_networkx_nodes(G, pos, node_size=node_size, node_color='lightblue', 
                              alpha=0.7, ax=ax)
        
        # Draw edges with weight-based thickness
        nx.draw_networkx_edges(G, pos, edgelist=edge_list, width=normalized_weights,
                              alpha=0.6, edge_color='gray', ax=ax)
        
        # Draw node labels with actual family names
        if node_names is not None and len(node_names) >= num_nodes:
            labels = {i: node_names[i][:15] + '...' if len(node_names[i]) > 15 else node_names[i] for i in range(num_nodes)}
        else:
            labels = {i: f"Node_{i}" for i in range(num_nodes)}
            
        nx.draw_networkx_labels(G, pos, labels, font_size=8, font_weight='bold', ax=ax)
        
        # Draw edge weights if requested
        if show_edge_weights and len(edge_list) > 0 and len(weights) > 0:
            edge_labels = {}
            for i, (u, v) in enumerate(edge_list):
                if i < len(weights):
                    # Show absolute edge weights with 3 decimal places
                    edge_labels[(u, v)] = f"{abs(weights[i]):.3f}"
            
            nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=6, ax=ax)
        
        ax.set_title(title, fontsize=12, fontweight='bold')
        ax.axis('off')
    
    def _save_individual_graph_plots(self):
        """Save individual high-resolution graph plots"""
        
        # Original graph
        plt.figure(figsize=(15, 15))
        ax = plt.gca()
        original_edges = self.dataset.original_graph_data['edge_index'].shape[1] // 2
        self._visualize_single_graph(
            self.dataset.original_graph_data['edge_index'],
            self.dataset.original_graph_data['edge_weight'],
            self.dataset.original_graph_data['edge_type'],
            ax,
            f"Original k-NN Graph ({len(self.dataset.node_feature_names)} nodes, {original_edges} edges)"
        )
        plt.savefig(f"{self.save_dir}/graphs/original_graph_highres.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… Individual high-res graphs saved to {self.save_dir}/graphs/")
    
    def create_tsne_visualization(self, model, data_list, target_idx=1):
        """NEW FEATURE 3: Create t-SNE visualization"""
        print(f"\n" + "="*60)
        print("ğŸ†• NEW FEATURE 3: t-SNE Embedding Visualization")
        print("="*60)
        
        # Extract embeddings
        model.eval()
        embeddings_list = []
        targets_list = []
        
        loader = DataLoader(data_list, batch_size=self.batch_size, shuffle=False)
        
        with torch.no_grad():
            for batch in loader:
                batch = batch.to(device)
                _, emb = model(batch.x, batch.edge_index, batch.batch)
                embeddings_list.append(emb.cpu().numpy())
                targets_list.append(batch.y.cpu().numpy())
        
        embeddings = np.vstack(embeddings_list)
        targets = np.vstack(targets_list)
        
        print(f"Extracted embeddings shape: {embeddings.shape}")
        
        # Create explainer for t-SNE functionality
        explainer = GNNExplainerRegression(model, device)
        
        # Save t-SNE plot
        tsne_path = f"{self.save_dir}/plots/H2-km_tsne_embeddings.png"
        tsne_coords = explainer.create_tsne_embedding_plot(
            embeddings,
            targets,
            target_names=self.dataset.target_cols,
            save_path=tsne_path
        )
        
        # Save embeddings
        np.save(f"{self.save_dir}/embeddings/H2-km_embeddings.npy", embeddings)
        np.save(f"{self.save_dir}/embeddings/H2-km_targets.npy", targets)
        
        return embeddings, targets, tsne_coords
    
    def run_full_pipeline_with_graphs(self):
        """Run the complete Case 1 pipeline with PROPER graph visualizations"""
        print("ğŸš€ Starting Case 1 Pipeline with COMPLETE Graph Visualizations")
        print("="*80)
        
        try:
            # Step 1: Create dataset
            dataset = self.create_anchored_dataset()
            
            # Step 2: Train initial models
            gnn_results, best_model = self.train_gnn_models(dataset.data_list)
            
            # Step 3: Create both sparsified graphs
            edge_based_data, edge_based_graph_data, node_based_data, node_based_graph_data, kept_nodes, pruned_node_names = self.create_both_sparsified_graphs(best_model)
            
            # Step 4: Create comprehensive graph visualizations
            graph_comparison_path = self.create_comprehensive_graph_visualizations(edge_based_graph_data, node_based_graph_data)
            
            # Step 5: Train model on node-based pruned graph
            print(f"\nTraining final model on node-pruned graph...")
            final_model = simple_GCN_res_plus_regression(
                hidden_channels=self.hidden_dim,
                dropout_prob=0.3,
                input_channel=1,
                output_dim=1
            ).to(device)
            
            # Quick training on pruned graph
            train_loader = DataLoader(node_based_data, batch_size=self.batch_size, shuffle=True)
            optimizer = Adam(final_model.parameters(), lr=self.learning_rate)
            criterion = nn.MSELoss()
            
            for epoch in range(self.num_epochs):
                final_model.train()
                for batch in train_loader:
                    batch = batch.to(device)
                    optimizer.zero_grad()
                    out, _ = final_model(batch.x, batch.edge_index, batch.batch)
                    target = batch.y[:, 1].view(-1, 1)  # H2-km target
                    loss = criterion(out, target)
                    loss.backward()
                    optimizer.step()
            
            # Step 6: Create t-SNE visualization
            embeddings, targets, tsne_coords = self.create_tsne_visualization(final_model, node_based_data)
            
            # Final results
            print(f"\n" + "ğŸ‰" * 30)
            print("âœ… COMPLETE CASE 1 PIPELINE WITH GRAPHS COMPLETED!")
            print("ğŸ‰" * 30)
            
            print(f"\nğŸ“Š Results Summary:")
            print(f"  â€¢ Original features: {len(dataset.node_feature_names)}")
            print(f"  â€¢ Node-pruned features: {len(kept_nodes)} ({len(kept_nodes)/len(dataset.node_feature_names)*100:.1f}% retained)")
            print(f"  â€¢ Original edges: {self.dataset.original_graph_data['edge_index'].shape[1]//2}")
            print(f"  â€¢ Edge-based edges: {edge_based_graph_data['edge_index'].shape[1]//2}")
            print(f"  â€¢ Node-based edges: {node_based_graph_data['edge_index'].shape[1]//2}")
            print(f"  â€¢ Embeddings shape: {embeddings.shape}")
            print(f"  â€¢ t-SNE coordinates shape: {tsne_coords.shape}")
            
            print(f"\nğŸ“ Files created in {self.save_dir}:")
            print(f"  âœ… graphs/comprehensive_graph_comparison.png - All 3 graphs side by side")
            print(f"  âœ… graphs/original_graph_highres.png - High-res original graph")
            print(f"  âœ… plots/H2-km_tsne_embeddings.png - t-SNE visualization") 
            print(f"  âœ… embeddings/H2-km_embeddings.npy - Node embeddings")
            print(f"  âœ… embeddings/H2-km_targets.npy - Target values")
            
            print(f"\nğŸ¯ COMPLETE FEATURES DEMONSTRATED:")
            print(f"  âœ… Original k-NN graph visualization")
            print(f"  âœ… Edge-based sparsified graph visualization") 
            print(f"  âœ… Node-based sparsified graph visualization")
            print(f"  âœ… Node importance reporting")
            print(f"  âœ… t-SNE embedding clustering visualization")
            print(f"  âœ… Consistent node/edge sizing across all graphs")
            
            return {
                'gnn_results': gnn_results,
                'edge_based_graph': edge_based_graph_data,
                'node_based_graph': node_based_graph_data,
                'kept_nodes': kept_nodes,
                'pruned_node_names': pruned_node_names,
                'embeddings': embeddings,
                'targets': targets,
                'tsne_coords': tsne_coords,
                'graph_comparison_path': graph_comparison_path
            }
            
        except Exception as e:
            print(f"âŒ Error in pipeline: {e}")
            import traceback
            traceback.print_exc()
            return None

def main():
    """Run Case 1 test with COMPLETE graph visualizations"""
    data_path = "../Data/New_Data.csv"
    
    # Check data file
    if not os.path.exists(data_path):
        print(f"âŒ Data file not found: {data_path}")
        return
    
    # Run Case 1 pipeline with graphs
    pipeline = Case1PipelineWithGraphs(data_path, save_dir="./case1_complete_graphs")
    results = pipeline.run_full_pipeline_with_graphs()
    
    if results:
        print(f"\nğŸŠ Case 1 with COMPLETE GRAPHS completed successfully!")
        print(f"Check ./case1_complete_graphs/graphs/ for all graph visualizations")
    else:
        print(f"âš ï¸  Pipeline encountered issues")

if __name__ == "__main__":
    main()