#!/usr/bin/env python3

"""
Case 1 Pipeline with PROPER Graph Visualizations
- Initial k-NN graph visualization
- Edge-based sparsified graph visualization  
- Node-based sparsified graph visualization
- All new features properly integrated
"""

import os
import sys
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from torch import nn
from torch.optim import Adam, lr_scheduler
from torch_geometric.loader import DataLoader
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.svm import LinearSVR
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import torch.nn.functional as F
from torch_geometric.data import Data
import pickle
import warnings
warnings.filterwarnings('ignore')

# Add src to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Import our modules
from datasets.dataset_regression import MicrobialGNNDataset
from explainers.explainer_regression import GNNExplainerRegression
from explainers.pipeline_explainer import create_explainer_sparsified_graph
from models.GNNmodelsRegression import (
    simple_GCN_res_plus_regression,
    simple_RGGC_plus_regression,
    simple_GAT_regression
)

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Reproducibility
torch.manual_seed(42)
np.random.seed(42)

class Case1PipelineWithGraphs:
    """Case 1: H2-km prediction with COMPLETE graph visualizations"""
    
    def __init__(self, data_path, save_dir="./case1_with_graphs_results"):
        self.data_path = data_path
        self.save_dir = save_dir
        
        # Create directories
        os.makedirs(f"{save_dir}/plots", exist_ok=True)
        os.makedirs(f"{save_dir}/models", exist_ok=True)
        os.makedirs(f"{save_dir}/embeddings", exist_ok=True)
        os.makedirs(f"{save_dir}/graphs", exist_ok=True)
        
        # Fast parameters for testing
        self.k_neighbors = 5
        self.hidden_dim = 32
        self.num_epochs = 15  # Very fast for testing
        self.num_folds = 2    # Fast
        self.batch_size = 4
        self.learning_rate = 0.01
        
        print(f"Case 1 Pipeline initialized - Results will be saved to: {save_dir}")
    
    def create_anchored_dataset(self):
        """Create dataset with Case 1 anchored features"""
        print("\n" + "="*60)
        print("STEP 1: Creating Case 1 Dataset with Anchored Features")
        print("="*60)
        
        # Case 1: Hydrogenotrophic families
        anchored_families = [
            'Methanoregulaceae',      # Hydrogenotrophic
            'Methanobacteriaceae',    # Hydrogenotrophic  
            'Methanospirillaceae'     # Hydrogenotrophic
        ]
        
        print(f"Case 1 anchored families: {anchored_families}")
        
        # Create base dataset
        self.dataset = MicrobialGNNDataset(
            data_path=self.data_path,
            k_neighbors=self.k_neighbors,
            mantel_threshold=0.05,
            use_fast_correlation=True,
            graph_mode='family',
            family_filter_mode='relaxed'
        )
        
        print(f"Dataset created with {len(self.dataset.data_list)} samples")
        print(f"Target variables: {self.dataset.target_cols}")
        print(f"Number of features: {len(self.dataset.node_feature_names)}")
        
        # IMPORTANT: Store original graph data for visualization
        self.dataset.original_graph_data = {
            'edge_index': self.dataset.data_list[0].edge_index.clone(),
            'edge_weight': torch.ones(self.dataset.data_list[0].edge_index.shape[1]),
            'edge_type': torch.ones(self.dataset.data_list[0].edge_index.shape[1], dtype=torch.long)
        }
        
        print(f"Original k-NN graph stored: {self.dataset.original_graph_data['edge_index'].shape[1]} edges")
        
        return self.dataset
    
    def train_gnn_models(self, data_list, target_idx=1):  # H2-km is index 1
        """Train GNN models on the data"""
        print(f"\n" + "="*60)
        print(f"STEP 2: Training GNN Models for {self.dataset.target_cols[target_idx]}")
        print("="*60)
        
        models_to_train = ['gcn']  # Just GCN for speed
        results = {}
        
        for model_type in models_to_train:
            print(f"\nTraining {model_type.upper()}...")
            
            # Create model
            model = simple_GCN_res_plus_regression(
                hidden_channels=self.hidden_dim,
                dropout_prob=0.3,
                input_channel=1,
                output_dim=1
            ).to(device)
            
            # Train with cross-validation
            kf = KFold(n_splits=self.num_folds, shuffle=True, random_state=42)
            fold_results = []
            best_model_state = None
            best_r2 = -float('inf')
            
            for fold, (train_idx, test_idx) in enumerate(kf.split(data_list), 1):
                print(f"  Fold {fold}/{self.num_folds}")
                
                train_data = [data_list[i] for i in train_idx]
                test_data = [data_list[i] for i in test_idx]
                
                train_loader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)
                test_loader = DataLoader(test_data, batch_size=self.batch_size, shuffle=False)
                
                optimizer = Adam(model.parameters(), lr=self.learning_rate)
                criterion = nn.MSELoss()
                
                # Training loop
                for epoch in range(self.num_epochs):
                    model.train()
                    for batch in train_loader:
                        batch = batch.to(device)
                        optimizer.zero_grad()
                        out, _ = model(batch.x, batch.edge_index, batch.batch)
                        target = batch.y[:, target_idx].view(-1, 1)
                        loss = criterion(out, target)
                        loss.backward()
                        optimizer.step()
                
                # Validation
                model.eval()
                preds, trues = [], []
                with torch.no_grad():
                    for batch in test_loader:
                        batch = batch.to(device)
                        out, _ = model(batch.x, batch.edge_index, batch.batch)
                        preds.append(out.cpu().numpy())
                        trues.append(batch.y[:, target_idx].cpu().numpy())
                
                preds = np.concatenate(preds).flatten()
                trues = np.concatenate(trues).flatten()
                
                r2 = r2_score(trues, preds)
                rmse = np.sqrt(mean_squared_error(trues, preds))
                
                fold_results.append({
                    'fold': fold,
                    'r2': r2,
                    'rmse': rmse,
                    'predictions': preds,
                    'targets': trues
                })
                
                if r2 > best_r2:
                    best_r2 = r2
                    best_model_state = model.state_dict().copy()
                
                print(f"    Fold {fold}: R¬≤ = {r2:.3f}, RMSE = {rmse:.3f}")
            
            # Load best model
            model.load_state_dict(best_model_state)
            
            overall_r2 = np.mean([f['r2'] for f in fold_results])
            overall_rmse = np.mean([f['rmse'] for f in fold_results])
            
            print(f"  {model_type.upper()} Overall: R¬≤ = {overall_r2:.3f}, RMSE = {overall_rmse:.3f}")
            
            results[model_type] = {
                'model': model,
                'fold_results': fold_results,
                'r2': overall_r2,
                'rmse': overall_rmse
            }
        
        # Get best model
        best_model = results['gcn']['model']
        
        return results, best_model
    
    def create_both_sparsified_graphs(self, model, target_idx=1):
        """Create BOTH edge-based and node-based sparsified graphs"""
        print(f"\n" + "="*60)
        print("STEP 3: Creating BOTH Sparsification Types")
        print("="*60)
        
        # Create explainer
        explainer = GNNExplainerRegression(model, device)
        
        # Generate explanations from multiple samples
        print("Generating explanations...")
        num_samples = min(5, len(self.dataset.data_list))
        combined_importance = torch.zeros(
            (len(self.dataset.node_feature_names), len(self.dataset.node_feature_names)), 
            device=device
        )
        
        for i in range(num_samples):
            data = self.dataset.data_list[i]
            edge_importance_matrix, _ = explainer.explain_graph(
                data, 
                node_names=self.dataset.node_feature_names,
                target_idx=target_idx
            )
            combined_importance += edge_importance_matrix
        
        combined_importance /= num_samples
        
        print(f"\nüÜï NEW FEATURE 1: Node Importance Reporting")
        print("-" * 50)
        
        # NEW FEATURE 1: Node importance reporting
        node_importance, sorted_indices = explainer.get_node_importance(
            combined_importance, 
            self.dataset.node_feature_names
        )
        
        # Create EDGE-BASED sparsification (original method)
        print(f"\nüìä Creating EDGE-BASED Sparsified Graph")
        print("-" * 50)
        
        edge_based_data = create_explainer_sparsified_graph(
            pipeline=self,
            model=model,
            target_idx=target_idx,
            importance_threshold=0.2,
            use_node_pruning=False  # Use original edge-based method
        )
        
        # Store edge-based graph data for visualization
        edge_based_graph_data = self.dataset.explainer_sparsified_graph_data.copy()
        
        print(f"\nüÜï Creating NODE-BASED Sparsified Graph")
        print("-" * 50)
        
        # Create NODE-BASED sparsification (new method)
        template_data = self.dataset.data_list[0]
        pruned_data, kept_nodes, pruned_node_names = explainer.create_node_pruned_graph(
            template_data,
            combined_importance,
            self.dataset.node_feature_names,
            importance_threshold=0.2,  # Adjust to get reasonable pruning
            min_nodes=50  # Ensure we keep at least 50 nodes
        )
        
        # Create node-based dataset
        node_based_data = []
        feature_matrix_samples = self.dataset.feature_matrix.T
        
        for s in range(feature_matrix_samples.shape[0]):
            x = torch.tensor(feature_matrix_samples[s][kept_nodes], dtype=torch.float32).view(-1, 1)
            targets = torch.tensor(self.dataset.target_df.iloc[s].values, dtype=torch.float32).view(1, -1)
            
            data = Data(
                x=x,
                edge_index=pruned_data.edge_index.clone(),
                y=targets
            )
            node_based_data.append(data)
        
        # Store node-based graph data for visualization
        node_based_graph_data = {
            'edge_index': pruned_data.edge_index.clone(),
            'edge_weight': torch.ones(pruned_data.edge_index.shape[1]),
            'edge_type': torch.ones(pruned_data.edge_index.shape[1], dtype=torch.long),
            'pruning_type': 'node_based',
            'kept_nodes': kept_nodes,
            'pruned_node_names': pruned_node_names
        }
        
        return edge_based_data, edge_based_graph_data, node_based_data, node_based_graph_data, kept_nodes, pruned_node_names
    
    def create_comprehensive_graph_visualizations(self, edge_based_graph_data, node_based_graph_data):
        """Create comprehensive graph visualizations"""
        print(f"\n" + "="*60)
        print("üé® STEP 4: Creating Comprehensive Graph Visualizations")
        print("="*60)
        
        # Create a figure with three subplots
        fig, axes = plt.subplots(1, 3, figsize=(30, 10))
        
        num_original_nodes = len(self.dataset.node_feature_names)
        
        # 1. Original k-NN graph
        original_edges = self.dataset.original_graph_data['edge_index'].shape[1] // 2
        self._visualize_single_graph(
            self.dataset.original_graph_data['edge_index'],
            self.dataset.original_graph_data['edge_weight'],
            self.dataset.original_graph_data['edge_type'],
            axes[0],
            title=f"Original k-NN Graph\n({num_original_nodes} nodes, {original_edges} edges)"
        )
        
        # 2. Edge-based sparsified graph
        edge_based_edges = edge_based_graph_data['edge_index'].shape[1] // 2
        edge_retention = (edge_based_edges / original_edges * 100) if original_edges > 0 else 0
        self._visualize_single_graph(
            edge_based_graph_data['edge_index'],
            edge_based_graph_data['edge_weight'],
            edge_based_graph_data['edge_type'],
            axes[1],
            title=f"Edge-Based Sparsified\n({num_original_nodes} nodes, {edge_based_edges} edges)\n({edge_retention:.1f}% edges retained)"
        )
        
        # 3. Node-based sparsified graph
        node_based_nodes = len(node_based_graph_data['kept_nodes'])
        node_based_edges = node_based_graph_data['edge_index'].shape[1] // 2
        node_retention = (node_based_nodes / num_original_nodes * 100)
        edge_retention_node = (node_based_edges / original_edges * 100) if original_edges > 0 else 0
        self._visualize_single_graph(
            node_based_graph_data['edge_index'],
            node_based_graph_data['edge_weight'],
            node_based_graph_data['edge_type'],
            axes[2],
            title=f"Node-Based Sparsified\n({node_based_nodes} nodes, {node_based_edges} edges)\n({node_retention:.1f}% nodes, {edge_retention_node:.1f}% edges retained)"
        )
        
        plt.tight_layout()
        graph_comparison_path = f"{self.save_dir}/graphs/comprehensive_graph_comparison.png"
        plt.savefig(graph_comparison_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"‚úÖ Comprehensive graph comparison saved: {graph_comparison_path}")
        
        # Create individual high-res graphs
        self._save_individual_graph_plots()
        
        return graph_comparison_path
    
    def _visualize_single_graph(self, edge_index, edge_weight, edge_type, ax, title):
        """Visualize a single graph with consistent node/edge sizing"""
        import networkx as nx
        from matplotlib.colors import LinearSegmentedColormap
        
        # Convert to NetworkX
        G = nx.Graph()
        
        # Add nodes
        num_nodes = edge_index.max().item() + 1
        G.add_nodes_from(range(num_nodes))
        
        # Add edges with weights
        edge_list = []
        weights = []
        for i in range(edge_index.shape[1]):
            u, v = edge_index[0, i].item(), edge_index[1, i].item()
            if u != v:  # Avoid self-loops
                weight = edge_weight[i].item() if len(edge_weight) > i else 1.0
                edge_list.append((u, v))
                weights.append(weight)
                G.add_edge(u, v, weight=weight)
        
        if len(edge_list) == 0:
            ax.text(0.5, 0.5, "No edges to display", ha='center', va='center', fontsize=14)
            ax.set_title(title)
            ax.axis('off')
            return
        
        # Create layout
        try:
            pos = nx.spring_layout(G, k=1, iterations=50, seed=42)
        except:
            pos = {i: (np.random.random(), np.random.random()) for i in G.nodes()}
        
        # Normalize edge weights for consistent visualization
        if weights:
            min_weight = min(weights)
            max_weight = max(weights)
            if max_weight > min_weight:
                normalized_weights = [(w - min_weight) / (max_weight - min_weight) * 3 + 0.5 for w in weights]
            else:
                normalized_weights = [1.0] * len(weights)
        else:
            normalized_weights = [1.0]
        
        # Draw nodes with consistent sizing
        node_size = max(10, 300 - len(G.nodes()) // 10)  # Scale node size based on graph size
        nx.draw_networkx_nodes(G, pos, node_size=node_size, node_color='lightblue', 
                              alpha=0.7, ax=ax)
        
        # Draw edges with weight-based thickness
        nx.draw_networkx_edges(G, pos, edgelist=edge_list, width=normalized_weights,
                              alpha=0.6, edge_color='gray', ax=ax)
        
        ax.set_title(title, fontsize=12, fontweight='bold')
        ax.axis('off')
    
    def _save_individual_graph_plots(self):
        """Save individual high-resolution graph plots"""
        
        # Original graph
        plt.figure(figsize=(15, 15))
        ax = plt.gca()
        original_edges = self.dataset.original_graph_data['edge_index'].shape[1] // 2
        self._visualize_single_graph(
            self.dataset.original_graph_data['edge_index'],
            self.dataset.original_graph_data['edge_weight'],
            self.dataset.original_graph_data['edge_type'],
            ax,
            f"Original k-NN Graph ({len(self.dataset.node_feature_names)} nodes, {original_edges} edges)"
        )
        plt.savefig(f"{self.save_dir}/graphs/original_graph_highres.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"‚úÖ Individual high-res graphs saved to {self.save_dir}/graphs/")
    
    def create_tsne_visualization(self, model, data_list, target_idx=1):
        """NEW FEATURE 3: Create t-SNE visualization"""
        print(f"\n" + "="*60)
        print("üÜï NEW FEATURE 3: t-SNE Embedding Visualization")
        print("="*60)
        
        # Extract embeddings
        model.eval()
        embeddings_list = []
        targets_list = []
        
        loader = DataLoader(data_list, batch_size=self.batch_size, shuffle=False)
        
        with torch.no_grad():
            for batch in loader:
                batch = batch.to(device)
                _, emb = model(batch.x, batch.edge_index, batch.batch)
                embeddings_list.append(emb.cpu().numpy())
                targets_list.append(batch.y.cpu().numpy())
        
        embeddings = np.vstack(embeddings_list)
        targets = np.vstack(targets_list)
        
        print(f"Extracted embeddings shape: {embeddings.shape}")
        
        # Create explainer for t-SNE functionality
        explainer = GNNExplainerRegression(model, device)
        
        # Save t-SNE plot
        tsne_path = f"{self.save_dir}/plots/H2-km_tsne_embeddings.png"
        tsne_coords = explainer.create_tsne_embedding_plot(
            embeddings,
            targets,
            target_names=self.dataset.target_cols,
            save_path=tsne_path
        )
        
        # Save embeddings
        np.save(f"{self.save_dir}/embeddings/H2-km_embeddings.npy", embeddings)
        np.save(f"{self.save_dir}/embeddings/H2-km_targets.npy", targets)
        
        return embeddings, targets, tsne_coords
    
    def run_full_pipeline_with_graphs(self):
        """Run the complete Case 1 pipeline with PROPER graph visualizations"""
        print("üöÄ Starting Case 1 Pipeline with COMPLETE Graph Visualizations")
        print("="*80)
        
        try:
            # Step 1: Create dataset
            dataset = self.create_anchored_dataset()
            
            # Step 2: Train initial models
            gnn_results, best_model = self.train_gnn_models(dataset.data_list)
            
            # Step 3: Create both sparsified graphs
            edge_based_data, edge_based_graph_data, node_based_data, node_based_graph_data, kept_nodes, pruned_node_names = self.create_both_sparsified_graphs(best_model)
            
            # Step 4: Create comprehensive graph visualizations
            graph_comparison_path = self.create_comprehensive_graph_visualizations(edge_based_graph_data, node_based_graph_data)
            
            # Step 5: Train model on node-based pruned graph
            print(f"\nTraining final model on node-pruned graph...")
            final_model = simple_GCN_res_plus_regression(
                hidden_channels=self.hidden_dim,
                dropout_prob=0.3,
                input_channel=1,
                output_dim=1
            ).to(device)
            
            # Quick training on pruned graph
            train_loader = DataLoader(node_based_data, batch_size=self.batch_size, shuffle=True)
            optimizer = Adam(final_model.parameters(), lr=self.learning_rate)
            criterion = nn.MSELoss()
            
            for epoch in range(self.num_epochs):
                final_model.train()
                for batch in train_loader:
                    batch = batch.to(device)
                    optimizer.zero_grad()
                    out, _ = final_model(batch.x, batch.edge_index, batch.batch)
                    target = batch.y[:, 1].view(-1, 1)  # H2-km target
                    loss = criterion(out, target)
                    loss.backward()
                    optimizer.step()
            
            # Step 6: Create t-SNE visualization
            embeddings, targets, tsne_coords = self.create_tsne_visualization(final_model, node_based_data)
            
            # Final results
            print(f"\n" + "üéâ" * 30)
            print("‚úÖ COMPLETE CASE 1 PIPELINE WITH GRAPHS COMPLETED!")
            print("üéâ" * 30)
            
            print(f"\nüìä Results Summary:")
            print(f"  ‚Ä¢ Original features: {len(dataset.node_feature_names)}")
            print(f"  ‚Ä¢ Node-pruned features: {len(kept_nodes)} ({len(kept_nodes)/len(dataset.node_feature_names)*100:.1f}% retained)")
            print(f"  ‚Ä¢ Original edges: {self.dataset.original_graph_data['edge_index'].shape[1]//2}")
            print(f"  ‚Ä¢ Edge-based edges: {edge_based_graph_data['edge_index'].shape[1]//2}")
            print(f"  ‚Ä¢ Node-based edges: {node_based_graph_data['edge_index'].shape[1]//2}")
            print(f"  ‚Ä¢ Embeddings shape: {embeddings.shape}")
            print(f"  ‚Ä¢ t-SNE coordinates shape: {tsne_coords.shape}")
            
            print(f"\nüìÅ Files created in {self.save_dir}:")
            print(f"  ‚úÖ graphs/comprehensive_graph_comparison.png - All 3 graphs side by side")
            print(f"  ‚úÖ graphs/original_graph_highres.png - High-res original graph")
            print(f"  ‚úÖ plots/H2-km_tsne_embeddings.png - t-SNE visualization") 
            print(f"  ‚úÖ embeddings/H2-km_embeddings.npy - Node embeddings")
            print(f"  ‚úÖ embeddings/H2-km_targets.npy - Target values")
            
            print(f"\nüéØ COMPLETE FEATURES DEMONSTRATED:")
            print(f"  ‚úÖ Original k-NN graph visualization")
            print(f"  ‚úÖ Edge-based sparsified graph visualization") 
            print(f"  ‚úÖ Node-based sparsified graph visualization")
            print(f"  ‚úÖ Node importance reporting")
            print(f"  ‚úÖ t-SNE embedding clustering visualization")
            print(f"  ‚úÖ Consistent node/edge sizing across all graphs")
            
            return {
                'gnn_results': gnn_results,
                'edge_based_graph': edge_based_graph_data,
                'node_based_graph': node_based_graph_data,
                'kept_nodes': kept_nodes,
                'pruned_node_names': pruned_node_names,
                'embeddings': embeddings,
                'targets': targets,
                'tsne_coords': tsne_coords,
                'graph_comparison_path': graph_comparison_path
            }
            
        except Exception as e:
            print(f"‚ùå Error in pipeline: {e}")
            import traceback
            traceback.print_exc()
            return None

def main():
    """Run Case 1 test with COMPLETE graph visualizations"""
    data_path = "../Data/New_Data.csv"
    
    # Check data file
    if not os.path.exists(data_path):
        print(f"‚ùå Data file not found: {data_path}")
        return
    
    # Run Case 1 pipeline with graphs
    pipeline = Case1PipelineWithGraphs(data_path, save_dir="./case1_complete_graphs")
    results = pipeline.run_full_pipeline_with_graphs()
    
    if results:
        print(f"\nüéä Case 1 with COMPLETE GRAPHS completed successfully!")
        print(f"Check ./case1_complete_graphs/graphs/ for all graph visualizations")
    else:
        print(f"‚ö†Ô∏è  Pipeline encountered issues")

if __name__ == "__main__":
    main()